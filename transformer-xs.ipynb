{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Setup GPU\n",
    "We only need one GPU for training and inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, copy, time, sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"# Using pytorch v{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CUDA\n",
    "Checking is cuda is available. I haven't dared to trained this on CPU. Even just using GPU also takes quite a lot of time to train well. In case cuda isn't detected in your system, you might not have a GPU or have the CPU variant of pytorch installed. You can always run this on Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"# Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Wikitext-103 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -c https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip wikitext-103-v1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder\n",
    "Holds the word embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoder\n",
    "Transformer doesn't have any sequential notion in it's architecture by default. So, it can only realize it's input as a bag of tokens. So, we need to explicitly provide positional information through the token embedding itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:,:seq_len], \\\n",
    "        requires_grad=False).cuda()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "This is core part of the transformer architecture. A single layer of Multi-Head Attention applies self-attention to all of it's inputs. The input of this operation is a bag of k tokens (each with it's representation of query, key and value) and output is updated representation of the k tokens again. Based on the query representation of every token, one first decide weights (or attention) for key representation of all other tokens. The updated output representation of the query token is constructed by taking linear combination of value representation of tokens using the weights calculated.\n",
    "\n",
    "In this implementation, we only look behind the current location by masking the indices ahead. This is because we want to predict the next word conditioned on the context behind. \n",
    "\n",
    "I plan to add the permutation language model functionality based on XLNet to allow learning bidirectional features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    # q, k, v : shape(bs, heads, L_max, d_k)\n",
    "    # scores: matmul [shape(bs,heads,L_max,d_k), shape(bs,heads,d_k,L_max)] -> shape(bs,heads,L_max,L_max)\n",
    "    # scores x v : shape(bs,heads,L_max,L_max) X shape(bs,heads,L_max,d_k) -> shape(bs,heads,L_max,d_k)\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # print(f\"### Shape of pre-softmax logits: {scores.shape}\")\n",
    "        # mask = mask.unsqueeze(1)\n",
    "        # print(f\"### Shape of mask: {mask.shape}\")\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "        \n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "A simple feed forward network with one hidden layer. Input and output dimensions are d_model and hidden layer size is d_ff (=2048 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "Following the trend in various papers, we also apply Layer Norm after every Multi Head attention and Feed Forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "Puts together a single layer of the Encoder. This applies [LayerNorm -> Multi-Head Attn -> LayerNorm -> Feed Forward] to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "    \n",
    "    def forward(self, x, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Puts together the whole network by stacking \n",
    "- Word Embedding Matrix\n",
    "- Positional Encoder\n",
    "- N multihead attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "Final wrapper class for Transformer. Nothing but the Encoder layer along with a final linear projection layer, that projects the output representation to log probability of words in vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(trg_vocab, d_model, N, heads)\n",
    "        # self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, trg, trg_mask):\n",
    "        # e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.encoder(trg, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "        # we don't perform softmax on the output as this will be handled \n",
    "        # automatically by our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Vocab\n",
    "Loads a saved vocab class object with word2index and index2word functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang = pickle.load(open('./models/wiki103.large.lang', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit vocab size\n",
    "We only consider a vocab size of 40000 for now. This version of model is based on English words seen in training dataset. To decrease number of <unk> in dataset, I kept the vocab size a bit large. If we use bpe, the vocab size can be decreased while keeping better coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang.limitVocab(40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiDataset\n",
    "WikiDataset class for fetching samples from wikitext-103 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "    def __init__(self, split, max_len=MAX_LEN):\n",
    "        super(WikiDataset, self).__init__()\n",
    "        if split == 'train':\n",
    "            _file = 'data/wikitext-103/wikitext-103/wiki.train.tokens'\n",
    "            n_lines = 1801350\n",
    "        elif split==\"valid\":\n",
    "            _file = 'data/wikitext-103/wikitext-103/wiki.valid.tokens'\n",
    "            n_lines = 3760\n",
    "        elif split==\"test\":\n",
    "            _file = 'data/wikitext-103/wikitext-103/wiki.test.tokens'\n",
    "            n_lines = 4358\n",
    "        else:\n",
    "            raise Exception(f\"wrong split: {split}\")\n",
    "        print(\"File:\", _file)\n",
    "        print(\"Expected # of lines:\", n_lines)\n",
    "        self.data = []\n",
    "        with open(_file) as f:\n",
    "            for line in tqdm_notebook(f, total=n_lines):\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    el = en_lang.encodeSentence(line)\n",
    "                    if len(el) < max_len:\n",
    "                        el = el + [en_lang.iEOS] + [en_lang.iPAD]*(max_len - len(el) - 1)\n",
    "                    else:\n",
    "                        el = el[:(max_len - 1)] + [en_lang.iEOS]\n",
    "                    self.data.append(el)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikiDataset_valid = WikiDataset('valid')\n",
    "wikiDataset_test = WikiDataset('test')\n",
    "wikiDataset_train = WikiDataset('train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config\n",
    "+ d_model: Embedding dim of words\n",
    "+ heads: Number of heads used for multi-head attention\n",
    "+ N: Number of MHA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "heads = 32\n",
    "N = 8\n",
    "_vocab = en_lang.VOCAB_SIZE\n",
    "model = Transformer(_vocab, d_model, N, heads)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "# this code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "# See this blog for a mathematical explanation.\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"## Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDataloader_train = DataLoader(wikiDataset_train, batch_size=32)\n",
    "wikiDataloader_valid = DataLoader(wikiDataset_valid, batch_size=32)\n",
    "wikiDataloader_test = DataLoader(wikiDataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"## Steps per epoch {len(wikiDataloader_train.dataset)//wikiDataloader_train.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    epochs=14\n",
    "    print_every=50\n",
    "    \n",
    "    _ = model.train()\n",
    "    start = time.time()\n",
    "    temp = start\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, batch in enumerate(wikiDataloader_train):\n",
    "            batch = torch.stack(batch).to(device)\n",
    "            trg = batch.t()\n",
    "\n",
    "            # the French sentence we input has all words except\n",
    "            # the last, as it is using each word to predict the next\n",
    "            trg_input = trg[:, :-1]\n",
    "\n",
    "            # the words we are trying to predict\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "            trg_mask = torch.tensor(np.tril(\n",
    "                np.ones(\n",
    "                    (1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "            ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "\n",
    "            preds = model(trg_input, trg_mask)\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets, ignore_index=en_lang.iPAD)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            total_loss += loss.data.item()\n",
    "            if (i + 1) % print_every == 0:\n",
    "                loss_avg = total_loss / print_every\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, PPL = %8.2f, %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "                epoch + 1, i + 1, loss_avg, math.exp(loss_avg), time.time() - temp,\n",
    "                print_every))\n",
    "                total_loss = 0\n",
    "                temp = time.time()\n",
    "                # raise Exception(\"STOP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'epoch': epoch,\n",
    "#     'iter': i,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optim.state_dict(),\n",
    "#     'loss': loss\n",
    "# }, \"models/txl_wikitext103.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"models/txl_wikitext103.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Set\n",
    "The following function runs the model on the test or validation set. You can use this function to calculate perplexity on the validation or test set to compare. I didn't bother doing this as I was more interested in the contextual text generation task. That's the next function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_from = 6\n",
    "        for i, batch in enumerate(wikiDataloader_valid):\n",
    "            batch = torch.stack(batch).to(device)\n",
    "            trg = batch.t()\n",
    "            zl = None\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_mask_common = torch.tensor(np.tril(\n",
    "                    np.ones(\n",
    "                        (1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "            \n",
    "            # the words we are trying to predict\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            for j in range(start_from, MAX_LEN - 1):\n",
    "                # Predicting (j+1)th word\n",
    "                if zl is None:\n",
    "                    zl = torch.tensor(np.zeros((1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                                      , device=device).double()\n",
    "                    zl[..., :j] = 1\n",
    "\n",
    "                zl[..., :j] = 1\n",
    "\n",
    "                # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "                trg_mask =  trg_mask_common * zl\n",
    "\n",
    "                preds = F.softmax(model(trg_input, trg_mask)[...,j,:], dim=-1)\n",
    "#                 samples = torch.multinomial(preds, 1)[:,0]\n",
    "                samples = torch.argmax(preds, 1)\n",
    "                # samples = samples.view(trg_input.shape[0], -1)\n",
    "\n",
    "                trg[..., (j+1)] = samples\n",
    "                print(f\"{j}\", end=\"\\r\")\n",
    "            return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = sample_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_lang.decodeSentence(preds[16, :].cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text sampler\n",
    "Finally, the text generator function. This is inspired by the talktotransformer site. I was blown away by that site. Of course, the model here trained is not as good as the fine-tuned GPT-2 model used for talktotransformer, but this gives a good flavour of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_me(context, max_len = MAX_LEN):\n",
    "    model.eval()\n",
    "    context = torch.tensor(en_lang.encodeSentence(context)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        start_from = (context.shape[1] - 1)\n",
    "        # for i, batch in enumerate(wikiDataloader_valid):\n",
    "        trg_input = context.to(device)\n",
    "        # trg = batch.t()\n",
    "        zl = None\n",
    "        # trg_input = trg[:, :-1]\n",
    "        trg_input = F.pad(trg_input, (0, MAX_LEN - trg_input.shape[1]), \"constant\", en_lang.iEOS)\n",
    "        \n",
    "        trg_mask_common = torch.tensor(np.tril(\n",
    "                np.ones(\n",
    "                    (1, 1, MAX_LEN, MAX_LEN))\n",
    "            ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "\n",
    "        for j in range(start_from, MAX_LEN - 1):\n",
    "            # Predicting (j+1)th word\n",
    "            if zl is None:\n",
    "                zl = torch.tensor(np.zeros((1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                                  , device=device).double()\n",
    "                zl[..., :j] = 1\n",
    "\n",
    "            zl[..., :j] = 1\n",
    "\n",
    "            # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "            trg_mask =  trg_mask_common * zl\n",
    "\n",
    "            preds = F.softmax(model(trg_input, trg_mask)[...,j,:], dim=-1)\n",
    "            if np.random.rand() < 0.2:\n",
    "                samples = torch.multinomial(preds, 1)[:,0]\n",
    "            else:\n",
    "                samples = torch.argmax(preds, 1)\n",
    "            # samples = samples.view(trg_input.shape[0], -1)\n",
    "\n",
    "            trg_input[..., (j+1)] = samples\n",
    "            if samples.item() == en_lang.iEOS:\n",
    "                return trg_input\n",
    "            print(f\"{j}\", end=\"\\r\")\n",
    "        return trg_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Barack Obama\"\n",
    "for i in range(5):\n",
    "    gen_text = talk_to_me(query)\n",
    "    print(f\"Sample {i}: \", ' '.join(en_lang.decodeSentence(gen_text.cpu()[0].numpy().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
