{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### Setup GPU\n",
    "We only need one GPU for training and inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, regex, math, copy, time, sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found utils.py...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./utils.py'):\n",
    "    print(\"Downloading utils.py...\")\n",
    "    url = \"https://raw.githubusercontent.com/bsantraigi/Transformer-XS/master/utils.py\"\n",
    "    import subprocess\n",
    "    subprocess.run([\"wget\", url])\n",
    "else:\n",
    "    print(\"Found utils.py...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Using pytorch v1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"# Using pytorch v{torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check CUDA\n",
    "Checking is cuda is available. I haven't dared to trained this on CPU. Even just using GPU also takes quite a lot of time to train well. In case cuda isn't detected in your system, you might not have a GPU or have the CPU variant of pytorch installed. You can always run this on Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"# Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Wikitext-103 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(\"data/wikitext-103/\"):\n",
    "    print(\"Downloading data...\")\n",
    "    subprocess.run(\"wget -c https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip -P data\".split())\n",
    "    print(\"Unzipping data...\")\n",
    "    subprocess.run([\"unzip\", \"data/wikitext-103-v1.zip\", \"-d\", \"data/\"])\n",
    "    print(\"Done...\")\n",
    "else:\n",
    "    print(\"Found data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Main target here is to create the VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/wikitext-103/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = 1801350\n",
    "test_lines = 4358\n",
    "valid_lines = 3760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = defaultdict(int)\n",
    "split = 'train'\n",
    "L = eval(f'{split}_lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Creation (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_path + f'wiki.{split}.tokens') as f:\n",
    "#     _progress = 0\n",
    "#     buffer = []\n",
    "#     for line in tqdm_notebook(f, total=L):\n",
    "#         # _progress += 1\n",
    "#         line = buffer.append(line.strip())\n",
    "#         # print(f'{_progress/L*100:2.2F}', end='\\r')\n",
    "#         if len(buffer) > 40000:\n",
    "#             buffer = ' '.join(buffer)\n",
    "#             tokens = list(en.tokenizer(buffer.lower()))\n",
    "#             buffer = []\n",
    "#             for w in tokens:\n",
    "#                 vocab[w.text] += 1\n",
    "    \n",
    "#     # One last time to clean the buffer\n",
    "#     buffer = ' '.join(buffer)\n",
    "#     tokens = list(en.tokenizer(buffer.lower()))\n",
    "#     buffer = []\n",
    "#     for w in tokens:\n",
    "#         vocab[w.text] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Load Vocab\n",
    "The following step will take some time, upto 10 mins. The spacy tokenizer is not as fast. But this is a one time process. Once the vocab file is created, you can just load from there.\n",
    "\n",
    "OR\n",
    "\n",
    "Loads a saved vocab class object with word2index and index2word functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of vocab might take upto 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocab file...\n",
      "CPU times: user 167 ms, sys: 53.9 ms, total: 221 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lang_file = \"./models/wiki103.large.lang\"\n",
    "if not os.path.isfile(lang_file):\n",
    "    print(\"Creating vocab file...\")\n",
    "    en_lang = Lang('wiki')\n",
    "    en_lang.buildLang(open(data_path + f'wiki.{split}.tokens'), num_lines=train_lines, care_for_newline=True)\n",
    "    with open(lang_file, 'wb') as f:\n",
    "        pickle.dump(en_lang, f)\n",
    "else:\n",
    "    print(\"Loading vocab file...\")\n",
    "    en_lang = pickle.load(open('./models/wiki103.large.lang', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit vocab size\n",
    "We only consider a vocab size of 40000 for now. This version of model is based on English words seen in training dataset. To decrease number of <unk> in dataset, I kept the vocab size a bit large. If we use bpe, the vocab size can be decreased while keeping better coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang.limitVocab(40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder\n",
    "Holds the word embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Positional Encoding\n",
    "Transformer doesn't have any sequential notion in it's architecture by default. So, it can only realize it's input as a bag of tokens. So, we need to explicitly provide positional information through the token embedding itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativePositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(2*max_seq_len, d_model, requires_grad=False)\n",
    "        \n",
    "        for pos in range(2*max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        \n",
    "        pe = pe.to(device)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, total_context_len):\n",
    "        return self.pe[:total_context_len,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "This is core part of the transformer architecture. A single layer of Multi-Head Attention applies self-attention to all of it's inputs. The input of this operation is a bag of k tokens (each with it's representation of query, key and value) and output is updated representation of the k tokens again. Based on the query representation of every token, one first decide weights (or attention) for key representation of all other tokens. The updated output representation of the query token is constructed by taking linear combination of value representation of tokens using the weights calculated.\n",
    "\n",
    "In this implementation, we only look behind the current location by masking the indices ahead. This is because we want to predict the next word conditioned on the context behind. \n",
    "\n",
    "I plan to add the permutation language model functionality based on XLNet to allow learning bidirectional features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.r_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \"\"\"\n",
    "        Bias parameters of relative positional encoding.\n",
    "        Ref - Transformer-XL paper\n",
    "        u, v \\in R^{heads, d_k}\n",
    "        \n",
    "        All heads will have different bias parameters.\n",
    "        For proper broadcasting, the shape of u and v\n",
    "        should be shape(1, heads, 1, d_k)\n",
    "        \"\"\"\n",
    "        self.u = torch.nn.Parameter(torch.randn(1, self.h, 1, self.d_k, device=device))\n",
    "        self.v = torch.nn.Parameter(torch.randn(1, self.h, 1, self.d_k, device=device))\n",
    "        \n",
    "        \"\"\"\n",
    "        For max seq length of MAX_LEN R_e will be able to support\n",
    "        a context of max length 2*MAX_LEN. This can be changed \n",
    "        without any need for retraining the model.\n",
    "        \"\"\"\n",
    "        max_seq_len = MAX_LEN\n",
    "        self.R = RelativePositionalEncoder(d_model, max_seq_len)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # R_{i-j} : Fixed component of the rel pos enc. coming from\n",
    "        # sinusoidal pos enc. \n",
    "        r = self.R(v.shape[1])\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        r = self.r_linear(r).view(1, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        r = r.transpose(1,2)\n",
    "        \n",
    "        # calculate attention using function we will define next\n",
    "        scores = self._attention(q, k, v, r, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "                    .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "    def _attention(self, q, k, v, r, d_k, mask=None, dropout=None):\n",
    "        # q, k, v : shape(bs, heads, L_max, d_k)\n",
    "        # scores: matmul [shape(bs,heads,L_max,d_k), shape(bs,heads,d_k,L_max)] -> shape(bs,heads,L_max,L_max)\n",
    "        # scores x v : shape(bs,heads,L_max,L_max) X shape(bs,heads,L_max,d_k) -> shape(bs,heads,L_max,d_k)\n",
    "        \n",
    "        scores = (\n",
    "            torch.matmul(q, k.transpose(-2, -1)) + \n",
    "            torch.matmul(q, r.transpose(-2, -1)) +\n",
    "            torch.matmul(self.u, k.transpose(-2, -1)) +\n",
    "            torch.matmul(self.v, r.transpose(-2, -1))\n",
    "        ) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # print(f\"### Shape of pre-softmax logits: {scores.shape}\")\n",
    "            # mask = mask.unsqueeze(1)\n",
    "            # print(f\"### Shape of mask: {mask.shape}\")\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        output = torch.matmul(scores, v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward\n",
    "A simple feed forward network with one hidden layer. Input and output dimensions are d_model and hidden layer size is d_ff (=2048 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "Following the trend in various papers, we also apply Layer Norm after every Multi Head attention and Feed Forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "Puts together a single layer of the Encoder. This applies [LayerNorm -> Multi-Head Attn -> LayerNorm -> Feed Forward] to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        # self.ext_mask = None\n",
    "    def forward(self, x, trg_mask, m=None, mem_mask=None):\n",
    "        x2 = self.norm_1(x)\n",
    "        state_2_save = x2 # We will return what was input to MHA as memory\n",
    "        if m is None:\n",
    "            keys = x2\n",
    "            values = x2\n",
    "        else:\n",
    "            keys = torch.cat((x2, m.detach()), dim=1)\n",
    "            values = keys\n",
    "            #if self.ext_mask is None:\n",
    "                # Define once\n",
    "            #    self.ext_mask = torch.tensor(np.ones(\n",
    "            #            trg_mask.shape), device=device)\n",
    "            trg_mask = torch.cat((mem_mask, trg_mask), dim=-1)\n",
    "\n",
    "        x = x + self.dropout_1(self.attn_1(x2, keys, values, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x, state_2_save.detach()\n",
    "\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "Puts together the whole network by stacking \n",
    "- Word Embedding Matrix\n",
    "- Positional Encoder\n",
    "- N multihead attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        # self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "        \n",
    "        self.memory = []\n",
    "    def forward(self, trg, trg_mask, mem_mask):\n",
    "        new_memory = []\n",
    "        x = self.embed(trg)\n",
    "        # x = self.pe(x)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            if len(self.memory) > 0:\n",
    "                x, icy_state = self.layers[i](x, trg_mask, self.memory[i], mem_mask)\n",
    "            else:\n",
    "                x, icy_state = self.layers[i](x, trg_mask)\n",
    "            new_memory.append(icy_state)\n",
    "        \n",
    "        self._reset_memory()\n",
    "            \n",
    "        self.memory = [l for l in new_memory]\n",
    "        return self.norm(x)\n",
    "    \n",
    "    def _reset_memory(self):\n",
    "        # Free up old memory\n",
    "        while len(self.memory) > 0:\n",
    "            _r = self.memory.pop(0)\n",
    "            del _r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer the Great\n",
    "Final wrapper class for Transformer. Nothing but the Encoder layer along with a final linear projection layer, that projects the output representation to log probability of words in vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(trg_vocab, d_model, N, heads)\n",
    "        # self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, trg, trg_mask, mem_mask):\n",
    "        \"\"\"\n",
    "        if no memory passed, a new context will start.\n",
    "        if memory is passed, will make use of that as key and value\n",
    "        \"\"\"\n",
    "        # e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.encoder(trg, trg_mask, mem_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "        # we don't perform softmax on the output as this will be handled \n",
    "        # automatically by our loss function\n",
    "        \n",
    "    def reset_memory(self):\n",
    "        self.encoder._reset_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WikiDataset\n",
    "WikiDataset class for fetching samples from wikitext-103 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDataset(Dataset):\n",
    "    \"\"\"An abstract class representing a Dataset.\n",
    "\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__len__``, that provides the size of the dataset, and ``__getitem__``,\n",
    "    supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "    def __init__(self, split, max_len=MAX_LEN):\n",
    "        super(WikiDataset, self).__init__()\n",
    "        if split == 'train':\n",
    "            _file = data_path + '/wiki.train.tokens'\n",
    "            n_lines = 1801350\n",
    "        elif split==\"valid\":\n",
    "            _file = data_path + '/wiki.valid.tokens'\n",
    "            n_lines = 3760\n",
    "        elif split==\"test\":\n",
    "            _file = data_path + '/wiki.test.tokens'\n",
    "            n_lines = 4358\n",
    "        else:\n",
    "            raise Exception(f\"wrong split: {split}\")\n",
    "        print(\"File:\", _file)\n",
    "        print(\"Expected # of lines:\", n_lines)\n",
    "        self.data = []\n",
    "        with open(_file) as f:\n",
    "            for line in tqdm_notebook(f, total=n_lines):\n",
    "                line = line.strip()\n",
    "                if len(line) > 0:\n",
    "                    el = en_lang.encodeSentence(line)\n",
    "                    if len(el) < max_len:\n",
    "                        el = el + [en_lang.iEOS] + [en_lang.iPAD]*(max_len - len(el) - 1)\n",
    "                    else:\n",
    "                        el = el[:(max_len - 1)] + [en_lang.iEOS]\n",
    "                    self.data.append(el)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contiguous WikiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contiguous_WikiDataset(Dataset):\n",
    "    \"\"\"Dataset loader \n",
    "    Assumes the files have following structure within a single file. \n",
    "    Each page is marked with one and only one `=` sign. Sections/sub-\n",
    "    sections might be marked with `==` or more of those.\n",
    "    Use this dataset class for context consistent batch-formation. \n",
    "    e.g. as required in Transformer-XL and XLNet\n",
    "    \n",
    "    >>>\n",
    "    = Page Title 1 =\n",
    "    \n",
    "    Page content\n",
    "    \n",
    "    = Page Title 2 =\n",
    "    \n",
    "    Page content\n",
    "    <<<\n",
    "    \n",
    "    :param: self.batch_size: Using with wikidataset class, not a very ideal thing to do. But \n",
    "    needed a workaround for now. \n",
    "    \"\"\"\n",
    "    def __init__(self, split, batch_size, max_len=MAX_LEN):\n",
    "        super(Contiguous_WikiDataset, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        SET PARAMETERs\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.split = split\n",
    "        self.max_len = MAX_LEN\n",
    "        \n",
    "        \"\"\"\n",
    "        Filenames\n",
    "        \"\"\"\n",
    "        if self.split == 'train':\n",
    "            self._file = data_path + '/wiki.train.tokens'\n",
    "            n_lines = 1801350\n",
    "        elif self.split==\"valid\":\n",
    "            self._file = data_path + '/wiki.valid.tokens'\n",
    "            n_lines = 3760\n",
    "        elif self.split==\"test\":\n",
    "            self._file = data_path + '/wiki.test.tokens'\n",
    "            n_lines = 4358\n",
    "        else:\n",
    "            raise Exception(f\"wrong self.split: {self.split}\")\n",
    "        print(\"File:\", self._file)\n",
    "        # print(\"Expected # of lines:\", n_lines)\n",
    "        \n",
    "        ## REGEX\n",
    "        self.re_page = regex.compile(r\"^ = [^=]+? = $\", regex.MULTILINE)\n",
    "        #self.re_page = regex.compile(r\"^ = ([^=]+)+? = *$\", regex.MULTILINE)\n",
    "        \n",
    "        \"\"\"\n",
    "        One time. pre-Loads the raw data\n",
    "        \"\"\"\n",
    "        self.load_data()\n",
    "        \n",
    "        \"\"\"\n",
    "        No shuffle initially:\n",
    "            This makes sure valid and test dataset is processed in \n",
    "            original order\n",
    "        \"\"\"\n",
    "        self.reShuffle(shuffle=False) \n",
    "        \n",
    "    def load_data(self):\n",
    "        _data = open(self._file).read()\n",
    "        \n",
    "        matches = [match.span()[0] for match in self.re_page.finditer(_data)]\n",
    "        self.N = len(matches)\n",
    "        print(self.N, 'documents found.')\n",
    "        \n",
    "        # Each entry in self.data is a document. Include\n",
    "        self.data = []\n",
    "        for start, end in tqdm_notebook(zip(matches, matches[1:] + [len(_data)]), total=self.N, desc=\"Encoding: \"):\n",
    "            self.data.append(\n",
    "                en_lang.encodeSentence(_data[start:end]) + [en_lang.iEOS]\n",
    "            )\n",
    "            \n",
    "    def reShuffle(self, shuffle=True):\n",
    "        if shuffle:\n",
    "            random.shuffle(self.data)\n",
    "        self.batched_data = []\n",
    "        next_doc = self.batch_size # next doc to pick if we ran out of text\n",
    "        current_docs = list(zip(range(self.batch_size), [0]*self.batch_size))\n",
    "        finished = 0\n",
    "        \n",
    "        # Batches tend to get very sparse near the end due to a few long seq\n",
    "        while(finished <= self.batch_size - 10):\n",
    "            print(f\"Status: {len(self.batched_data)} | {finished}/{self.N}\", end=\"\\r\")\n",
    "            # print(current_docs)\n",
    "            finished = 0\n",
    "            for i, (doc_x, doc_y) in enumerate(current_docs):\n",
    "                if doc_x != -1:\n",
    "                    reset = (doc_y+self.max_len) >= len(self.data[doc_x])\n",
    "                    el = self.data[doc_x][doc_y:(doc_y+self.max_len)]\n",
    "                    \n",
    "                    # PAD input seq if last sentence of document.\n",
    "                    el = el + [en_lang.iPAD]*(self.max_len - len(el))\n",
    "                    self.batched_data.append((el, reset))\n",
    "                    \n",
    "                    if reset:\n",
    "                        if next_doc < self.N:\n",
    "                            doc_x = next_doc\n",
    "                            next_doc += 1\n",
    "                            doc_y = 0\n",
    "                        else:\n",
    "                            doc_x = -1\n",
    "                    else:\n",
    "                        doc_y += self.max_len\n",
    "                    \n",
    "                    current_docs[i] = (doc_x, doc_y)\n",
    "                else:\n",
    "                    self.batched_data.append(([en_lang.iPAD]*self.max_len, True))\n",
    "                    finished += 1\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.batched_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batched_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: data/wikitext-103//wiki.valid.tokens\n",
      "60 documents found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1b58481ddf4f9d900423ea16a60a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Encoding: ', max=60, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Status: 0 | 0/60\r",
      "Status: 32 | 0/60\r",
      "Status: 64 | 0/60\r",
      "Status: 96 | 0/60\r",
      "Status: 128 | 0/60\r",
      "Status: 160 | 0/60\r",
      "Status: 192 | 0/60\r",
      "Status: 224 | 0/60\r",
      "Status: 256 | 0/60\r",
      "Status: 288 | 0/60\r",
      "Status: 320 | 0/60\r",
      "Status: 352 | 0/60\r",
      "Status: 384 | 0/60\r",
      "Status: 416 | 0/60\r",
      "Status: 448 | 0/60\r",
      "Status: 480 | 0/60\r",
      "Status: 512 | 0/60\r",
      "Status: 544 | 0/60\r",
      "Status: 576 | 0/60\r",
      "Status: 608 | 0/60\r",
      "Status: 640 | 0/60\r",
      "Status: 672 | 0/60\r",
      "Status: 704 | 0/60\r",
      "Status: 736 | 0/60\r",
      "Status: 768 | 0/60\r",
      "Status: 800 | 0/60\r",
      "Status: 832 | 0/60\r",
      "Status: 864 | 0/60\r",
      "Status: 896 | 0/60\r",
      "Status: 928 | 0/60\r",
      "Status: 960 | 0/60\r",
      "Status: 992 | 0/60\r",
      "Status: 1024 | 0/60\r",
      "Status: 1056 | 0/60\r",
      "Status: 1088 | 0/60\r",
      "Status: 1120 | 0/60\r",
      "Status: 1152 | 0/60\r",
      "Status: 1184 | 0/60\r",
      "Status: 1216 | 3/60\r",
      "Status: 1248 | 4/60\r",
      "Status: 1280 | 4/60\r",
      "Status: 1312 | 4/60\r",
      "Status: 1344 | 4/60\r",
      "Status: 1376 | 6/60\r",
      "Status: 1408 | 7/60\r",
      "Status: 1440 | 8/60\r",
      "Status: 1472 | 9/60\r",
      "Status: 1504 | 9/60\r",
      "Status: 1536 | 9/60\r",
      "Status: 1568 | 9/60\r",
      "Status: 1600 | 9/60\r",
      "Status: 1632 | 9/60\r",
      "Status: 1664 | 9/60\r",
      "Status: 1696 | 10/60\r",
      "Status: 1728 | 10/60\r",
      "Status: 1760 | 10/60\r",
      "Status: 1792 | 12/60\r",
      "Status: 1824 | 12/60\r",
      "Status: 1856 | 14/60\r",
      "Status: 1888 | 14/60\r",
      "Status: 1920 | 16/60\r",
      "Status: 1952 | 16/60\r",
      "Status: 1984 | 16/60\r",
      "Status: 2016 | 16/60\r",
      "Status: 2048 | 17/60\r",
      "Status: 2080 | 17/60\r",
      "Status: 2112 | 18/60\r",
      "Status: 2144 | 19/60\r",
      "Status: 2176 | 19/60\r",
      "Status: 2208 | 19/60\r",
      "Status: 2240 | 19/60\r",
      "Status: 2272 | 20/60\r",
      "Status: 2304 | 22/60\r",
      "Status: 2336 | 22/60\r",
      "Status: 2368 | 22/60\r",
      "File: data/wikitext-103//wiki.test.tokens\n",
      "64 documents found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d5dff81ad2420fa47477abc3e14371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Encoding: ', max=64, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: data/wikitext-103//wiki.train.tokens\n",
      "29863 documents found.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cf7fa8df07458eb8b74eb87d6c174c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Encoding: ', max=29863, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikiDataset_valid = Contiguous_WikiDataset('valid', BATCH_SIZE)\n",
    "wikiDataset_test = Contiguous_WikiDataset('test', BATCH_SIZE)\n",
    "wikiDataset_train = Contiguous_WikiDataset('train', BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiDataloader_train = DataLoader(wikiDataset_train, batch_size=BATCH_SIZE)\n",
    "# wikiDataloader_train = DataLoader(wikiDataset_valid, batch_size=BATCH_SIZE)\n",
    "wikiDataloader_valid = DataLoader(wikiDataset_valid, batch_size=BATCH_SIZE)\n",
    "wikiDataloader_test = DataLoader(wikiDataset_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"## Steps per epoch {len(wikiDataloader_train.dataset)//wikiDataloader_train.batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Config\n",
    "+ d_model: Embedding dim of words\n",
    "+ heads: Number of heads used for multi-head attention\n",
    "+ N: Number of MHA layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = 512\n",
    "# heads = 32\n",
    "# N = 8\n",
    "d_model = 256\n",
    "heads = 32\n",
    "N = 8\n",
    "_vocab = en_lang.VOCAB_SIZE\n",
    "model = Transformer(_vocab, d_model, N, heads)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "# this code is very important! It initialises the parameters with a\n",
    "# range of values that stops the signal fading or getting too big.\n",
    "# See this blog for a mathematical explanation.\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"## Training model with {pytorch_total_params/1000000:0.2F}M trainable parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def train_model():\n",
    "epochs=14\n",
    "print_every=50\n",
    "\n",
    "_ = model.train()\n",
    "start = time.time()\n",
    "temp = start\n",
    "\n",
    "total_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(wikiDataloader_train):\n",
    "        reset_keys = batch[1].double().to(device)\n",
    "        trg = torch.stack(batch[0]).t().to(device)\n",
    "        # raise Exception(\"WAIT!!\")\n",
    "        \n",
    "        # the French sentence we input has all words except\n",
    "        # the last, as it is using each word to predict the next\n",
    "        trg_input = trg[:, :-1]\n",
    "\n",
    "        # the words we are trying to predict\n",
    "        targets = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "        trg_mask = torch.tensor(np.tril(\n",
    "            np.ones(\n",
    "                (1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "        ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "        \n",
    "        mem_mask = torch.tensor(\n",
    "            np.ones(\n",
    "                (1, 1, trg_input.shape[1], trg_input.shape[1])\n",
    "            ), \n",
    "            device=device) * reset_keys[:,None, None, None]\n",
    "\n",
    "        preds = model(trg_input, trg_mask, mem_mask)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), targets, ignore_index=en_lang.iPAD)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        total_loss += loss.data.item()\n",
    "        if (i + 1) % print_every == 0:\n",
    "            loss_avg = total_loss / print_every\n",
    "            print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, PPL = %8.2f, %ds per %d iters\" % ((time.time() - start) // 60,\n",
    "            epoch + 1, i + 1, loss_avg, math.exp(loss_avg), time.time() - temp,\n",
    "            print_every))\n",
    "            total_loss = 0\n",
    "            temp = time.time()\n",
    "            # raise Exception(\"STOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.encoder.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Wait\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'epoch': epoch,\n",
    "#     'iter': i,\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'optimizer_state_dict': optim.state_dict(),\n",
    "#     'loss': loss\n",
    "# }, \"models/txl_wikitext103.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"models/txl_wikitext103.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Set\n",
    "The following function runs the model on the test or validation set. You can use this function to calculate perplexity on the validation or test set to compare. I didn't bother doing this as I was more interested in the contextual text generation task. That's the next function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequence():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_from = 6\n",
    "        for i, batch in enumerate(wikiDataloader_valid):\n",
    "            batch = torch.stack(batch).to(device)\n",
    "            trg = batch.t()\n",
    "            zl = None\n",
    "            trg_input = trg[:, :-1]\n",
    "            trg_mask_common = torch.tensor(np.tril(\n",
    "                    np.ones(\n",
    "                        (1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "            \n",
    "            # the words we are trying to predict\n",
    "            targets = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            for j in range(start_from, MAX_LEN - 1):\n",
    "                # Predicting (j+1)th word\n",
    "                if zl is None:\n",
    "                    zl = torch.tensor(np.zeros((1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                                      , device=device).double()\n",
    "                    zl[..., :j] = 1\n",
    "\n",
    "                zl[..., :j] = 1\n",
    "\n",
    "                # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "                trg_mask =  trg_mask_common * zl\n",
    "\n",
    "                preds = F.softmax(model(trg_input, trg_mask)[...,j,:], dim=-1)\n",
    "#                 samples = torch.multinomial(preds, 1)[:,0]\n",
    "                samples = torch.argmax(preds, 1)\n",
    "                # samples = samples.view(trg_input.shape[0], -1)\n",
    "\n",
    "                trg[..., (j+1)] = samples\n",
    "                print(f\"{j}\", end=\"\\r\")\n",
    "            return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = sample_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "en_lang.decodeSentence(preds[16, :].cpu().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text sampler\n",
    "Finally, the text generator function. This is inspired by the talktotransformer site. I was blown away by that site. Of course, the model here trained is not as good as the fine-tuned GPT-2 model used for talktotransformer, but this gives a good flavour of the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_to_me(context, max_len = MAX_LEN):\n",
    "    model.eval()\n",
    "    context = torch.tensor(en_lang.encodeSentence(context)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        start_from = (context.shape[1] - 1)\n",
    "        # for i, batch in enumerate(wikiDataloader_valid):\n",
    "        trg_input = context.to(device)\n",
    "        # trg = batch.t()\n",
    "        zl = None\n",
    "        # trg_input = trg[:, :-1]\n",
    "        trg_input = F.pad(trg_input, (0, MAX_LEN - trg_input.shape[1]), \"constant\", en_lang.iEOS)\n",
    "        \n",
    "        trg_mask_common = torch.tensor(np.tril(\n",
    "                np.ones(\n",
    "                    (1, 1, MAX_LEN, MAX_LEN))\n",
    "            ), device=device) * ((trg_input != en_lang.iPAD).double().unsqueeze(1).unsqueeze(1))\n",
    "\n",
    "        for j in range(start_from, MAX_LEN - 1):\n",
    "            # Predicting (j+1)th word\n",
    "            if zl is None:\n",
    "                zl = torch.tensor(np.zeros((1, 1, trg_input.shape[1], trg_input.shape[1]))\n",
    "                                  , device=device).double()\n",
    "                zl[..., :j] = 1\n",
    "\n",
    "            zl[..., :j] = 1\n",
    "\n",
    "            # create mask to make sure attn reads input only from the left (autoregressive)\n",
    "            trg_mask =  trg_mask_common * zl\n",
    "\n",
    "            preds = F.softmax(model(trg_input, trg_mask)[...,j,:], dim=-1)\n",
    "            if np.random.rand() < 0.2:\n",
    "                samples = torch.multinomial(preds, 1)[:,0]\n",
    "            else:\n",
    "                samples = torch.argmax(preds, 1)\n",
    "            # samples = samples.view(trg_input.shape[0], -1)\n",
    "\n",
    "            trg_input[..., (j+1)] = samples\n",
    "            if samples.item() == en_lang.iEOS:\n",
    "                return trg_input\n",
    "            print(f\"{j}\", end=\"\\r\")\n",
    "        return trg_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bangalore has the best\"\n",
    "for i in range(5):\n",
    "    gen_text = talk_to_me(query)\n",
    "    print(f\"Sample {i}: \", ' '.join(en_lang.decodeSentence(gen_text.cpu()[0].numpy().tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "489px",
    "width": "232px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "315px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
